{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to D:\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\inaugural.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('inaugural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'adventure',\n",
       " u'belles_lettres',\n",
       " u'editorial',\n",
       " u'fiction',\n",
       " u'government',\n",
       " u'hobbies',\n",
       " u'humor',\n",
       " u'learned',\n",
       " u'lore',\n",
       " u'mystery',\n",
       " u'news',\n",
       " u'religion',\n",
       " u'reviews',\n",
       " u'romance',\n",
       " u'science_fiction']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Thirty-three', u'Scotty', u'did', u'not', u'go', ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories=\"fiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1789-Washington.txt',\n",
       " u'1793-Washington.txt',\n",
       " u'1797-Adams.txt',\n",
       " u'1801-Jefferson.txt',\n",
       " u'1805-Jefferson.txt',\n",
       " u'1809-Madison.txt',\n",
       " u'1813-Madison.txt',\n",
       " u'1817-Monroe.txt',\n",
       " u'1821-Monroe.txt',\n",
       " u'1825-Adams.txt',\n",
       " u'1829-Jackson.txt',\n",
       " u'1833-Jackson.txt',\n",
       " u'1837-VanBuren.txt',\n",
       " u'1841-Harrison.txt',\n",
       " u'1845-Polk.txt',\n",
       " u'1849-Taylor.txt',\n",
       " u'1853-Pierce.txt',\n",
       " u'1857-Buchanan.txt',\n",
       " u'1861-Lincoln.txt',\n",
       " u'1865-Lincoln.txt',\n",
       " u'1869-Grant.txt',\n",
       " u'1873-Grant.txt',\n",
       " u'1877-Hayes.txt',\n",
       " u'1881-Garfield.txt',\n",
       " u'1885-Cleveland.txt',\n",
       " u'1889-Harrison.txt',\n",
       " u'1893-Cleveland.txt',\n",
       " u'1897-McKinley.txt',\n",
       " u'1901-McKinley.txt',\n",
       " u'1905-Roosevelt.txt',\n",
       " u'1909-Taft.txt',\n",
       " u'1913-Wilson.txt',\n",
       " u'1917-Wilson.txt',\n",
       " u'1921-Harding.txt',\n",
       " u'1925-Coolidge.txt',\n",
       " u'1929-Hoover.txt',\n",
       " u'1933-Roosevelt.txt',\n",
       " u'1937-Roosevelt.txt',\n",
       " u'1941-Roosevelt.txt',\n",
       " u'1945-Roosevelt.txt',\n",
       " u'1949-Truman.txt',\n",
       " u'1953-Eisenhower.txt',\n",
       " u'1957-Eisenhower.txt',\n",
       " u'1961-Kennedy.txt',\n",
       " u'1965-Johnson.txt',\n",
       " u'1969-Nixon.txt',\n",
       " u'1973-Nixon.txt',\n",
       " u'1977-Carter.txt',\n",
       " u'1981-Reagan.txt',\n",
       " u'1985-Reagan.txt',\n",
       " u'1989-Bush.txt',\n",
       " u'1993-Clinton.txt',\n",
       " u'1997-Clinton.txt',\n",
       " u'2001-Bush.txt',\n",
       " u'2005-Bush.txt',\n",
       " u'2009-Obama.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'My', u'fellow', u'citizens', u':', u'I', u'stand', ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.words(fileids='2009-Obama.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unicode"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(u'hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(b'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b'a'+b'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to D:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u\"you're\",\n",
       " u\"you've\",\n",
       " u\"you'll\",\n",
       " u\"you'd\",\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves',\n",
       " u'he',\n",
       " u'him',\n",
       " u'his',\n",
       " u'himself',\n",
       " u'she',\n",
       " u\"she's\",\n",
       " u'her',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'it',\n",
       " u\"it's\",\n",
       " u'its',\n",
       " u'itself',\n",
       " u'they',\n",
       " u'them',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'themselves',\n",
       " u'what',\n",
       " u'which',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'this',\n",
       " u'that',\n",
       " u\"that'll\",\n",
       " u'these',\n",
       " u'those',\n",
       " u'am',\n",
       " u'is',\n",
       " u'are',\n",
       " u'was',\n",
       " u'were',\n",
       " u'be',\n",
       " u'been',\n",
       " u'being',\n",
       " u'have',\n",
       " u'has',\n",
       " u'had',\n",
       " u'having',\n",
       " u'do',\n",
       " u'does',\n",
       " u'did',\n",
       " u'doing',\n",
       " u'a',\n",
       " u'an',\n",
       " u'the',\n",
       " u'and',\n",
       " u'but',\n",
       " u'if',\n",
       " u'or',\n",
       " u'because',\n",
       " u'as',\n",
       " u'until',\n",
       " u'while',\n",
       " u'of',\n",
       " u'at',\n",
       " u'by',\n",
       " u'for',\n",
       " u'with',\n",
       " u'about',\n",
       " u'against',\n",
       " u'between',\n",
       " u'into',\n",
       " u'through',\n",
       " u'during',\n",
       " u'before',\n",
       " u'after',\n",
       " u'above',\n",
       " u'below',\n",
       " u'to',\n",
       " u'from',\n",
       " u'up',\n",
       " u'down',\n",
       " u'in',\n",
       " u'out',\n",
       " u'on',\n",
       " u'off',\n",
       " u'over',\n",
       " u'under',\n",
       " u'again',\n",
       " u'further',\n",
       " u'then',\n",
       " u'once',\n",
       " u'here',\n",
       " u'there',\n",
       " u'when',\n",
       " u'where',\n",
       " u'why',\n",
       " u'how',\n",
       " u'all',\n",
       " u'any',\n",
       " u'both',\n",
       " u'each',\n",
       " u'few',\n",
       " u'more',\n",
       " u'most',\n",
       " u'other',\n",
       " u'some',\n",
       " u'such',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'only',\n",
       " u'own',\n",
       " u'same',\n",
       " u'so',\n",
       " u'than',\n",
       " u'too',\n",
       " u'very',\n",
       " u's',\n",
       " u't',\n",
       " u'can',\n",
       " u'will',\n",
       " u'just',\n",
       " u'don',\n",
       " u\"don't\",\n",
       " u'should',\n",
       " u\"should've\",\n",
       " u'now',\n",
       " u'd',\n",
       " u'll',\n",
       " u'm',\n",
       " u'o',\n",
       " u're',\n",
       " u've',\n",
       " u'y',\n",
       " u'ain',\n",
       " u'aren',\n",
       " u\"aren't\",\n",
       " u'couldn',\n",
       " u\"couldn't\",\n",
       " u'didn',\n",
       " u\"didn't\",\n",
       " u'doesn',\n",
       " u\"doesn't\",\n",
       " u'hadn',\n",
       " u\"hadn't\",\n",
       " u'hasn',\n",
       " u\"hasn't\",\n",
       " u'haven',\n",
       " u\"haven't\",\n",
       " u'isn',\n",
       " u\"isn't\",\n",
       " u'ma',\n",
       " u'mightn',\n",
       " u\"mightn't\",\n",
       " u'mustn',\n",
       " u\"mustn't\",\n",
       " u'needn',\n",
       " u\"needn't\",\n",
       " u'shan',\n",
       " u\"shan't\",\n",
       " u'shouldn',\n",
       " u\"shouldn't\",\n",
       " u'wasn',\n",
       " u\"wasn't\",\n",
       " u'weren',\n",
       " u\"weren't\",\n",
       " u'won',\n",
       " u\"won't\",\n",
       " u'wouldn',\n",
       " u\"wouldn't\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133737"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries=nltk.corpus.cmudict.entries()\n",
    "len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'car', u'auto', u'automobile', u'machine', u'motorcar']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('May', 'NNP'), ('the', 'DT'), ('Force', 'NNP'), ('be', 'VB'), ('with', 'IN'), ('you', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "texts=[\"\"\"May the Force be with you.\"\"\"]\n",
    "for text in texts:\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        print tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"Apollo's\", u'surgery', u'has', u'begun', u'\\ud83d', u'\\ude4f']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "text = \"Apollo's surgery has begun 🙏\"\n",
    "twtkn = TweetTokenizer()\n",
    "twtkn.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     D:\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "die\n",
      "sing\n",
      "dy\n",
      "sing\n",
      "ride\n",
      "ridicul\n",
      "coffe\n",
      "restaur\n",
      "happi\n",
      "unchart\n",
      "unnot\n",
      "enchant\n",
      "bottom\n",
      "internet\n",
      "indian\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer1=PorterStemmer()\n",
    "print(stemmer1.stem('working'))\n",
    "print(stemmer1.stem('dying'))\n",
    "print(stemmer1.stem('sing'))\n",
    "print(stemmer1.stem('Dying'))\n",
    "print(stemmer1.stem('singing'))\n",
    "print(stemmer1.stem('riding'))\n",
    "print(stemmer1.stem('ridicule'))\n",
    "print(stemmer1.stem('coffee'))\n",
    "print(stemmer1.stem('restaurant'))\n",
    "print(stemmer1.stem('happiness'))\n",
    "print(stemmer1.stem('uncharted'))\n",
    "print(stemmer1.stem('unnoticed'))\n",
    "print(stemmer1.stem('enchant'))\n",
    "print(stemmer1.stem('bottom'))\n",
    "print(stemmer1.stem('internet'))\n",
    "print(stemmer1.stem('Indian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(u'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dying\n",
      "sing\n",
      "ridic\n",
      "coff\n",
      "resta\n",
      "happy\n",
      "unchart\n",
      "unnot\n",
      "ench\n",
      "ench\n",
      "bottom\n",
      "internet\n",
      "ind\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer2=LancasterStemmer()\n",
    "print(stemmer2.stem('dying'))\n",
    "print(stemmer2.stem('singing'))\n",
    "print(stemmer2.stem('ridicule'))\n",
    "print(stemmer2.stem('coffee'))\n",
    "print(stemmer2.stem('restaurant'))\n",
    "print(stemmer2.stem('happiness'))\n",
    "print(stemmer2.stem('uncharted'))\n",
    "print(stemmer2.stem('unnoticed'))\n",
    "print(stemmer2.stem('enchant'))\n",
    "print(stemmer2.stem('enchanted'))\n",
    "print(stemmer2.stem('bottom'))\n",
    "print(stemmer2.stem('internet'))\n",
    "print(stemmer2.stem('Indian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "die\n",
      "restaur\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer3 = SnowballStemmer('english')\n",
    "print(stemmer3.stem('dying'))\n",
    "print(stemmer3.stem('restaurant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sii\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "stemmer4 = RegexpStemmer('ng')\n",
    "print(stemmer4.stem('singing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat wa chase a mous\n"
     ]
    }
   ],
   "source": [
    "stemmer= PorterStemmer()\n",
    "example = \"A cat was chasing a mouse\"\n",
    "example = [stemmer.stem(token) for token in example.split(\" \")]\n",
    "print(\" \".join(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat wa chasing the mice. There wa cactus around the corner.\n",
      "A cat wa chasing the mouse\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "example1 = \"A cat was chasing the mice. There was cacti around the corner.\"\n",
    "example2 = \"A cat was chasing the mice\"\n",
    "example1 = [lemmatizer.lemmatize(token) for token in example1.split(\" \")]\n",
    "example2 = [lemmatizer.lemmatize(token) for token in example2.split(\" \")]\n",
    "print(\" \".join(example1))\n",
    "print(\" \".join(example2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('better',pos='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.44610081 0.3174044  0.44610081 0.3174044  0.3174044  0.3174044\n",
      "  0.         0.44610081]]\n",
      "[[0.44610081 0.3174044  0.44610081 0.3174044  0.3174044  0.3174044\n",
      "  0.         0.44610081]\n",
      " [0.         0.4090901  0.         0.4090901  0.4090901  0.4090901\n",
      "  0.57496187 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "vect= TfidfVectorizer(binary = True)\n",
    "corpus=[\"Tesseract is an optical character recognition engine\",\"optical character recognition is significant\"]\n",
    "vect.fit(corpus)\n",
    "print(vect.transform([\"Tesseract is an optical character recognition engine\"]).toarray())\n",
    "print(vect.transform(corpus).toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07518868 0.         0.07518868 0.07518868 0.         0.07518868\n",
      "  0.         0.07518868 0.10699473 0.07518868 0.         0.07518868\n",
      "  0.07518868 0.07518868 0.07518868 0.         0.07518868 0.07518868\n",
      "  0.07518868 0.07518868 0.         0.07518868 0.         0.\n",
      "  0.07518868 0.07518868 0.07518868 0.         0.         0.\n",
      "  0.         0.         0.07518868 0.07518868 0.         0.07518868\n",
      "  0.07518868 0.         0.10699473 0.         0.07518868 0.07518868\n",
      "  0.         0.07518868 0.07518868 0.07518868 0.15037737 0.15037737\n",
      "  0.         0.         0.1604921  0.07518868 0.         0.05349737\n",
      "  0.07518868 0.07518868 0.         0.22556605 0.07518868 0.\n",
      "  0.         0.         0.         0.07518868 0.         0.07518868\n",
      "  0.15037737 0.         0.         0.         0.3209842  0.\n",
      "  0.07518868 0.         0.         0.15037737 0.05349737 0.\n",
      "  0.07518868 0.07518868 0.         0.07518868 0.07518868 0.07518868\n",
      "  0.         0.         0.07518868 0.         0.         0.07518868\n",
      "  0.07518868 0.07518868 0.07518868 0.07518868 0.07518868 0.07518868\n",
      "  0.         0.07518868 0.07518868 0.07518868 0.07518868 0.\n",
      "  0.         0.07518868 0.         0.         0.         0.\n",
      "  0.07518868 0.07518868 0.05349737 0.48147631 0.05349737 0.\n",
      "  0.07518868 0.07518868 0.         0.07518868 0.10699473 0.07518868\n",
      "  0.07518868 0.         0.         0.07518868 0.         0.07518868\n",
      "  0.07518868 0.15037737 0.07518868 0.07518868 0.         0.22556605\n",
      "  0.         0.07518868 0.07518868 0.07518868]\n",
      " [0.         0.09458364 0.         0.         0.09458364 0.\n",
      "  0.09458364 0.         0.06729704 0.         0.09458364 0.\n",
      "  0.         0.         0.         0.09458364 0.         0.\n",
      "  0.         0.         0.09458364 0.         0.09458364 0.09458364\n",
      "  0.         0.         0.         0.09458364 0.09458364 0.09458364\n",
      "  0.09458364 0.09458364 0.         0.         0.09458364 0.\n",
      "  0.         0.09458364 0.06729704 0.09458364 0.         0.\n",
      "  0.09458364 0.         0.         0.         0.         0.\n",
      "  0.09458364 0.09458364 0.06729704 0.         0.28375093 0.13459408\n",
      "  0.         0.         0.09458364 0.         0.         0.18916729\n",
      "  0.09458364 0.09458364 0.09458364 0.         0.09458364 0.\n",
      "  0.         0.09458364 0.09458364 0.09458364 0.20189113 0.09458364\n",
      "  0.         0.09458364 0.28375093 0.         0.06729704 0.09458364\n",
      "  0.         0.         0.09458364 0.         0.         0.\n",
      "  0.09458364 0.09458364 0.         0.09458364 0.09458364 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.09458364 0.         0.         0.         0.         0.09458364\n",
      "  0.09458364 0.         0.09458364 0.09458364 0.09458364 0.09458364\n",
      "  0.         0.         0.13459408 0.13459408 0.13459408 0.09458364\n",
      "  0.         0.         0.09458364 0.         0.26918817 0.\n",
      "  0.         0.37833458 0.18916729 0.         0.09458364 0.\n",
      "  0.         0.         0.         0.         0.09458364 0.\n",
      "  0.09458364 0.         0.         0.        ]]\n",
      "[[1.         0.20881245]\n",
      " [0.20881245 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "vect=TfidfVectorizer(binary=False)\n",
    "corpus=[\"\"\"One of the finer books I read this year was John Kaag's Hiking With Nietzsche, in which Kaag, a professor of philosophy, rekindles his passion for the German thinker while tracing picturesque hiking trails in the mountains of Switzerland. It's a near-precise rendering of the travelogue as a self-help book. A young Kaag was an avowed Nietzsche acolyte but given the ravages of responsibilities and adulthood, the writer put affinity to test by undertaking physically enduring hikes through the Alps, revisiting haunts that the philosopher escaped to, in search of solitude and salve. The journey's demands, coupled with his own inner turmoil, are catnip for anybody feeling at cross purposes with their own life.\"\"\",\"\"\"If there is a phrase I would prefer to retire from online bios, personal or professional, it is, \"I love travel\". Or some approximation of that sentiment. To clarify, I am not against travellers or those who proudly flaunt their passion for travel. On the contrary, editing a travel magazine has now made me oddly protective of travellers and their ilk. My submission is that \"love to travel\" , suggested so casually, just doesnt feel adequate to the depth of emotion it sparks in true devotees.\"\"\"]\n",
    "vect.fit(corpus)\n",
    "similarity=cosine_similarity(vect.transform(corpus))\n",
    "print(vect.transform(corpus).toarray())\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
